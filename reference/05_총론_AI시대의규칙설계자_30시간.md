# AI 시대의 규칙 설계자: 30시간 교육과정

## "실패에서 규칙을 뽑아내는 사람"을 기르는 교육

---

## 1. 교육과정의 핵심 아이디어

### 한 문장 요약

> 학생이 AI에게 다양한 일을 시키고, AI가 실수할 때마다 규칙을 한 줄씩 추가하여,
> 30시간 후 **"실패에서 태어난 나만의 AI 규칙 파일"**을 완성합니다.

### 이 교육이 길러주는 것

AI가 더 똑똑해질수록, 더 중요해지는 능력이 있습니다:

- AI에게 **무엇을 시킬지** 결정하는 것
- AI의 결과가 **괜찮은지** 판단하는 것
- AI가 실수했을 때 **다시는 같은 실수를 못 하게** 만드는 것

이 세 가지를 합치면, 학생은 AI의 "사용자"가 아니라 **"규칙 설계자"**가 됩니다.

### 이론적 배경

이 교육과정은 세 가지 통찰이 만나는 지점에 서 있습니다.

**Hashimoto의 실천** — 실리콘밸리 개발자가 AI에게 코딩을 시키면서 6개월에 걸쳐 규칙 파일(AGENTS.md)을 만든 여정:

> "이 파일의 모든 줄은 에이전트가 실제로 저지른 나쁜 행동에 기반해서 추가되었다."

**Fowler의 통찰** — "엄격함의 재배치(Relocating Rigor)": 기술이 발전하면 엄격함이 사라지는 것이 아니라 이동한다:

> "생성이 더 쉬워진다면, 판단은 더 엄격해져야 한다."

**Böckeler의 프레임** — AI와 협업하는 시스템(하네스)의 세 구성요소:
1. 컨텍스트 엔지니어링 → 교육에서: **문제 구조화 능력**
2. 아키텍처 제약 → 교육에서: **수용 기준을 미리 세우는 능력**
3. 가비지 컬렉션 → 교육에서: **반복적 점검과 개선 루틴**

이 세 관점이 수렴하는 핵심: **AI 시대에 엄격함은 코드 타이핑에서 의도 명세와 결과 검증으로 이동했고, 이 이동한 엄격함을 학생이 직접 체험하고 자기 것으로 만드는 것**이 교육의 목표입니다.

### 교실 용어: "규칙 파일"

이 교육과정에서는 Hashimoto의 AGENTS.md, Böckeler의 하네스를 모두 **"규칙 파일"**이라는 교실 친화적 용어로 통일합니다. 

규칙 파일이란:
- AI가 실수할 때마다 한 줄씩 추가하는 문서
- 각 줄에는 "어떤 실패가 있었기에 이 규칙이 태어났는지"가 기록됨
- 사전에 계획해서 쓰는 것이 아니라, **실패 경험에서만 자라남**
- 도메인이 바뀌어도 같은 구조가 적용됨 (챗봇, VPython, 데이터 분석, Pico 모두)

---

## 2. 대상 및 전제 조건

| 항목 | 내용 |
|------|------|
| 대상 | 고등학교 1학년 (일반고) |
| 전제 지식 | 중학교 정보 수업 경험 (Python 기초), AI 도구 사용 경험은 불문 |
| 교과 연계 | 2022 개정 교육과정 「정보」 「정보과학」 |
| 수업 형태 | 주 2시간, 15주 (1학기 기준) |
| 장비 | 개인 노트북 또는 학교 컴퓨터, 인터넷 접속, Pico 2W 키트 (트랙 C) |

---

## 3. 30시간 전체 구조

```
┌──────────────────────────────────────────────────────────────────────┐
│                         30시간 전체 구조                               │
├─────────────┬─────────────┬─────────────┬─────────────┬────────────┤
│  도입부      │  트랙 A     │  트랙 B     │  트랙 C     │  종합      │
│ (4시간)     │ (8시간)     │ (8시간)     │ (8시간)     │ (2시간)    │
│              │             │             │             │            │
│ 나만의 봇   │ VPython     │ 일상 문제   │ Pico 2W     │ 통합 규칙  │
│ 만들기      │ 3D창작·     │ 해결        │ 피지컬      │ 파일       │
│ + 첫 규칙   │ 시뮬레이션  │ 데이터 분석 │ 컴퓨팅      │ 완성       │
│ 파일        │             │             │             │            │
├─────────────┴─────────────┴─────────────┴─────────────┴────────────┤
│                                                                      │
│  검증 방법의 진행:                                                    │
│  대화 상식 → 시각 → 시각+수치 → 논리 → 물리적 동작                    │
│                                                                      │
│  규칙 파일의 성장:                                                    │
│  ~5개 → ~15개 → ~25개 → ~35개 → 통합·정제                           │
└──────────────────────────────────────────────────────────────────────┘
```

### 트랙 순서의 설계 근거

트랙 순서는 **"검증의 진입 장벽"**과 **"AI가 충분히 틀리는가"**로 결정했습니다.

| 순서 | 트랙 | 검증에 필요한 것 | AI 실수 빈도 | "이상한데?" 감각 |
|------|------|-----------------|-------------|-----------------|
| 1번 | 도입 (챗봇) | 대화 상식 | 높음 | 즉각적 |
| 2번 | A (VPython) | 눈 (3D) → 물리 공식 (시뮬) | 높음 | 시각적 → 수치적 |
| 3번 | B (데이터 분석) | 일상적 판단력 + 논리 | 매우 높음 | 직감적 → 논리적 |
| 4번 | C (Pico 2W) | 하드웨어 + 코드 + 환경 | 기초에서 낮음 | 물리적 동작 |

**도입부(챗봇) 직후에는 트랙 A(VPython)가 가장 자연스럽습니다.**

VPython 3D 조형은 진입 장벽이 가장 낮습니다. "로봇 만들어줘"라고 AI에게 시키면 팔이 머리에서 나오거나 좌우 비대칭이 됩니다. 물리 공식이나 통계 지식 없이 **눈으로 보면** "이건 아닌데"를 즉시 압니다. 그리고 명세를 좌표로 정밀하게 쓰면 결과가 극적으로 좋아지는 경험이 "명세의 위력"을 가장 직관적으로 체감시킵니다.

트랙 B(데이터 분석)가 두 번째인 이유는, "정답이 없는 영역에서 스스로 기준을 세우는" 것이 세 트랙 중 가장 높은 수준의 비판적 사고를 요구하기 때문입니다. 트랙 A에서 "기준을 세우고 → 결과를 판정하고 → 규칙을 축적하는" 구조를 명확한 시각/수치 기준으로 연습한 뒤에야, 트랙 B에서 "논리적 타당성"이라는 더 모호한 기준을 세울 준비가 됩니다.

트랙 C(Pico)가 마지막인 이유는 두 가지입니다. 첫째, 기초 과제에서 AI가 너무 잘해서 규칙을 만들 동기가 생기지 않습니다. 둘째, 실패 원인이 하드웨어/코드/환경 세 곳에 분산되어 원인 분리가 어렵습니다. 하지만 앞의 두 트랙을 경험한 학생은 "기초에서 잘한다고 방심하면 안 된다"는 감각과 "AI 문제 vs 다른 문제를 분리하는" 능력이 있으므로 대처할 수 있습니다.

### 전체를 관통하는 핵심 루프

30시간 동안 모든 트랙에서 같은 루프가 반복됩니다:

```
[1] 직접 해보기 (AI 없이)
         ↓
[2] AI에게 같은 일 시키기
         ↓
[3] 내 것과 AI 것을 비교하고 판단하기
         ↓
[4] "이건 돼야 하고, 저건 안 돼도 돼" 수용 기준 세우기
         ↓
[5] AI가 실수하면 규칙 추가하기
         ↓
     (처음으로, 다음 과제에서)
```

이 루프는 Hashimoto의 6단계 여정을 교실용으로 압축한 것입니다:

| Hashimoto 단계 | 교실 루프 | 핵심 |
|---------------|-----------|------|
| 1. 챗봇을 버려라 | [1] 직접 해보기 | AI 없이의 기준선 확보 |
| 2. 내 일을 재현하라 | [2] AI에게 시키기 | 동일 과제의 비교 체험 |
| 3. 하루 마감 에이전트 | [3] 비교·판단 | 역량 경계 파악 |
| 4. 쉬운 일은 외주 줘라 | [4] 수용 기준 세우기 | 선택적 위임 설계 |
| 5. 하네스를 설계하라 | [5] 규칙 추가 | 실패 기반 시스템 구축 |

---

## 4. 도입부: 나만의 봇 만들기 (4시간)

### 이 단계의 역할

도입부는 30시간 교육의 **관문**입니다. 여기서 학생이 체득해야 할 것은 딱 하나:

> **"AI가 실수하면, 규칙을 추가하면, 행동이 바뀐다"**

이 루프를 한 번 체험하면, 이후 트랙 A·B·C에서 "아, 도메인만 다르고 같은 걸 하는 거구나"를 즉시 이해합니다.

### 왜 챗봇인가

챗봇은 도입부 활동으로 이상적입니다:
- **사전 지식이 필요 없습니다.** 봇과 대화해보면 즉시 "이건 아닌데"를 느낍니다.
- **규칙 한 줄의 효과가 즉시 보입니다.** SYSTEM_PROMPT에 규칙 추가 → 새 대화 → 즉시 변화 확인.
- **코딩이 필요 없습니다.** 스타터 코드에서 SYSTEM_PROMPT 텍스트만 수정하면 됩니다.
- **각자 다른 봇을 만들어** 다양한 실패 패턴이 교실에 동시에 발생합니다.

### 플랫폼: Streamlit Cloud

GitHub 계정 + 브라우저만 있으면 됩니다. 선생님이 템플릿 저장소를 만들어두면, 학생은 Fork → Streamlit Cloud 배포 → SYSTEM_PROMPT 수정 → 자동 재배포.

API 키는 **선생님이 직접 관리**합니다. 수업 시작 전에 Anthropic 콘솔에서 spending limit이 설정된 키를 생성하고, Streamlit Cloud의 Secrets에 등록합니다. 학생에게 키를 직접 노출하지 않습니다.

### 4시간 상세 구성

#### 차시 1-2 (2시간): 체험 — "규칙 없는 봇은 이렇게 망가진다"

| 시간 | 활동 | 핵심 |
|------|------|------|
| 00-10분 | 선생님 시연: 규칙 1줄짜리 봇의 참사 | "이 봇 뭐가 문제예요?" |
| 10-15분 | "규칙 파일" 개념 도입 + Hashimoto 소개 | 실패 → 규칙 → 재시도 루프 |
| 15-20분 | 봇 주제 선정 (예시 목록에서) | 5분 이상 쓰지 않기 |
| 20-25분 | 환경 세팅 (Fork → 배포) | GitHub + Streamlit Cloud |
| 25-30분 | 첫 SYSTEM_PROMPT 작성 — "대충" | 의도적으로 규칙 최소화 |
| 30-48분 | 봇 테스트 + 실패 기록 (종이에) | 실패 기록 양식 사용 |
| 48-50분 | 실패 기록 → 규칙 파일로 옮기기 | 첫 3~4개 규칙 탄생 |

실패 기록 양식:
```
┌─────────────────────────────────────────────┐
│ 실패 기록 #___                               │
│ 내가 한 말: _______________________________  │
│ 봇이 한 말: _______________________________  │
│ 뭐가 이상한가: ____________________________  │
│ → 추가할 규칙: ____________________________  │
│   카테고리: □DON'T □DO □스타일 □범위         │
└─────────────────────────────────────────────┘
```

#### 차시 3-4 (2시간): 심화 — "남의 봇을 깨뜨리고, 공통 규칙을 발견한다"

| 시간 | 활동 | 핵심 |
|------|------|------|
| 00-05분 | 루프 재확인, 교차 테스트 예고 | |
| 05-25분 | 심화 테스트 + 규칙 축적 (자유 작업) | 선생님이 스트레스 테스트 시나리오 제안 |
| 25-40분 | 교차 테스트: 봇 URL 교환, 상대 봇 깨뜨리기 | 포스트잇으로 문제 전달 |
| 40-45분 | 교차 테스트 결과를 규칙에 반영 | |
| 45-50분 | 미니 갤러리 워크: 빨간 포스트잇(공통 규칙) 모으기 | "도메인을 넘는 규칙" 첫 발견 |

**도입부의 산출물:**
- 작동하는 Streamlit 챗봇 (URL)
- 규칙 5~10개가 담긴 SYSTEM_PROMPT
- 종이 실패 기록 3~4장
- "AI에게 뭘 시키든 적용되는" 공통 규칙 1~2개 발견

**도입부가 끝나면 학생에게 남는 감각:**
> "AI에게 시키기만 하면 안 되는구나. 규칙을 줘야 하는구나. 
> 그 규칙은 AI가 실수할 때 생기는구나."

---



## 5. 트랙 A: VPython — 3D 창작과 시뮬레이션 (8시간)

### 왜 첫 번째 트랙인가

도입부(챗봇)에서 "대화 상식으로 검증"을 체험한 직후, 검증 수준을 한 단계 올립니다:

```
도입부의 검증: "이 봇 이상해" → 대화 상식으로 검증
     ↓
트랙 A의 검증: "이 모양이 내가 원한 건 아닌데" → 눈으로 검증
                  "이 값이 이론과 다른데" → 수치로 검증
```

VPython 3D 조형은 도입부 다음으로 진입 장벽이 가장 낮습니다. "로봇 만들어줘"라고 AI에게 시키면 팔이 몸통을 관통하거나 좌우 비대칭이 됩니다. 물리 공식 없이 **눈으로** 즉시 검증 가능합니다.

동시에 **명세의 정밀도와 결과 품질의 관계가 극적으로 드러납니다.** "로봇 만들어줘"와 "빨간 box(2×3×1) 중심 (0,0,0), 파란 sphere(r=0.8) 중심 (0, 2.3, 0)..."의 결과 차이가 크기 때문입니다. Fowler의 "의도의 정밀한 명세"를 가장 직관적으로 체감시키는 트랙입니다.

### VPython의 세 수준

VPython은 물리 시뮬레이션 도구로만 알려져 있지만, 세 가지 수준의 활동을 지원합니다:

```
수준 1: 3D 조형          수준 2: 애니메이션         수준 3: 물리 시뮬레이션
로봇, 놀이터, 마을       움직임 부여               물리 법칙 적용

검증: 눈 (시각적)        검증: 눈 + 의도            검증: 눈 + 수치
진입 장벽: 최저          진입 장벽: 낮음            진입 장벽: 중간
AI 실수 빈도: 높음       AI 실수 빈도: 높음         AI 실수 빈도: 중간
```

트랙 A는 수준 1에서 시작하여 수준 3까지 진행합니다. **규칙을 만드는 구조는 동일하지만, 검증 방법이 점점 정교해집니다.**

### 3D 조형이 강력한 이유

"빨간 몸통에 파란 팔이 달린 로봇을 만들어줘"라고 AI에게 시키면, 팔이 몸통을 관통하거나 좌우 비대칭이거나 비율이 이상합니다. 학생은 물리 공식 없이 **눈으로** "아니 팔이 왜 머리에서 나와?"를 즉시 검증합니다.

더 중요한 것은, **명세의 정밀도와 결과 품질의 관계가 극적으로 드러나는** 점입니다:

```
모호한 요청:  "로봇 만들어줘"
→ 내가 원하던 것과 전혀 다름

구조화된 명세: "빨간 box(2×3×1) 중심 (0,0,0), 파란 sphere(r=0.8) 중심 (0, 2.3, 0)..."
→ 내가 원하던 것에 훨씬 가까움
```

### 학습 목표 (트랙 A 버전)

| 공통 목표 | 수준 1 (3D 조형) | 수준 2-3 (애니메이션·시뮬레이션) |
|----------|-----------------|-------------------------------|
| 1. AI 생성물 한계 식별 | 시각적 오류 발견 (위치, 비율, 색상) | 동작/물리 오류 발견 |
| 2. 활용 방식별 품질 차이 | 모호한 요청 vs 좌표 명세 요청의 차이 | 동작 명세의 정밀도에 따른 차이 |
| 3. AI 사용 여부 판단 | 전체 구상은 직접, 코드 구현은 AI에게 | 물리 개념 이해는 직접, 구현은 AI에게 |
| 4. 구조화된 명세 변환 | "로봇"을 객체·좌표·속성으로 구조화 | "공 던지기"를 조건·동작·검증으로 구조화 |
| 5. 수용 기준 사전 정의 | "팔은 몸통 양옆에 대칭"같은 시각 기준 | "도달 거리 이론값 ±5%"같은 수치 기준 |
| 6. 실패 기반 규칙 구축 | "AI는 좌표에서 y축과 z축을 혼동한다" | "AI는 dt를 너무 크게 잡는다" |
| 7. 메타인지적 점검 | 완성 장면을 명세서와 대조 | 시뮬레이션을 이론값과 대조 |

### 8시간 차시별 구성

#### 차시 1-2: 3D 조형 — "내 머릿속 그림을 명세로 바꾸기" (2시간)

| 시간 | 활동 | 핵심 |
|------|------|------|
| 00-10분 | 종이에 "만들고 싶은 로봇" 그리기 | 이 그림 = 학생의 의도 |
| 10-20분 | AI에게 "VPython으로 로봇 만들어줘" 요청 | 모호한 요청 |
| 20-35분 | 내 그림 vs AI 결과 비교표 작성 | 일치도 1~5점 (대부분 1~2점) |
| 35-50분 | 구조화된 명세(좌표, 크기, 색상)로 재요청 | 결과가 극적으로 개선됨 |
| 50-70분 | 수용 기준 세우기 (필수/선호 구분) | "관통 금지", "좌우 대칭" 등 |
| 70-90분 | AI 결과에 수용 기준 적용 | 합격/불합격 판정 |
| 90-100분 | 규칙 파일에 VPython 3D 규칙 추가 | "좌표를 명시해야 한다" 등 |

#### 차시 3-4: 애니메이션 — "움직임까지 명세하기" (2시간)

정적 조형에 움직임을 추가합니다. AI가 애니메이션에서 더 자주 실수합니다: 회전축 오류, 관통하며 움직이기, 속도 불일치.

| 시간 | 활동 | 핵심 |
|------|------|------|
| 00-30분 | 자유 장면 명세서 → AI 요청 → 수용 기준 검증 | 전체 사이클 두 번째 경험 |
| 30-50분 | "이제 움직이게 해보자" — 애니메이션 요청 | AI 실수 급증 지점 |
| 50-75분 | 동작 명세서 작성 연습 (회전축, 피벗, 범위) | |
| 75-95분 | 동작 명세서로 AI 재요청 + 비교 | 모호 vs 구조화 차이 재확인 |
| 95-100분 | 규칙 파일에 애니메이션 규칙 추가 | "회전축을 명시해야 한다" 등 |

#### 차시 5-6: 물리 시뮬레이션 — "이제 수치로 검증한다" (2시간)

검증이 시각에서 수치로 올라갑니다. 물리 공식은 교사가 제공하며, 학생은 공식을 **검증 도구**로 사용합니다.

| 시간 | 활동 | 핵심 |
|------|------|------|
| 00-15분 | 포물선 운동 손 계산 (공식 제공) | 이론값 = 검증 기준 |
| 15-30분 | AI에게 시뮬레이션 요청 (학생이 직접 명세) | 이미 명세 습관 있음 |
| 30-50분 | 수치적 수용 기준 체크 | "이론값 40.8m, 시뮬 결과 ___m" |
| 50-70분 | 자유 시뮬레이션 주제 선택 + 명세서 작성 | 진자, 충돌, 궤도 등 |
| 70-95분 | AI 요청 → 수치 검증 → 수정 요청 | |
| 95-100분 | 규칙 파일 업데이트 | "이론값과 대조 필수" 등 |

#### 차시 7-8: 자유 프로젝트 + 트랙 A 규칙 정리 (2시간)

학생이 세 수준(조형/애니메이션/시뮬레이션) 중 선택하여 자유 프로젝트를 수행합니다. **어떤 수준을 선택하든 평가 기준은 동일합니다.** 물리 시뮬레이션을 선택한 학생이 유리한 것이 아니라, 자기 수준에서 명세와 검증을 얼마나 정밀하게 했느냐가 중요합니다.

**규칙 파일 중간 리뷰 (차시 7-8 시작 전):** 선생님이 학생 규칙 파일을 훑어보고, 규칙의 질을 점검합니다. 좋은 규칙(구체적 사건 + 검증 가능한 조건)과 모호한 규칙("잘 해야 한다" 수준)의 차이를 2~3개 예시로 보여주고, 학생이 자기 규칙 파일을 개선할 시간을 5~10분 줍니다. 이 리뷰 활동이 트랙 B·C에서의 규칙 품질을 크게 높입니다.

**트랙 A 종료 시 규칙 파일 규모:** 도입부 5~10개 + 트랙 A 추가 5~10개 = 약 10~20개

---

## 6. 트랙 B: 일상 문제해결 — 데이터 분석 (8시간)

### 왜 두 번째 트랙인가

트랙 A에서 시각적·수치적으로 명확한 기준을 세우는 경험을 한 후, 트랙 B에서는 **"정답이 없는 영역에서 스스로 기준을 세우는"** 도전을 만납니다.

```
트랙 A의 검증: "이 모양이 맞는가?" "이 값이 이론과 일치하는가?" (시각/수치 — 명확한 기준)
     ↓
트랙 B의 검증: "이 분석에 논리적 비약이 없는가?" "이 결론이 타당한가?" (논리 — 모호한 기준)
```

트랙 A에서 "기준을 세우고 → 결과를 판정하고 → 규칙을 축적하는" 구조를 명확한 기준으로 연습했기 때문에, 이제 더 모호한 영역에서도 같은 구조를 적용할 준비가 되어 있습니다.

동시에 트랙 B는 AI가 가장 자주 실수하는 영역입니다. 상관관계를 인과관계로 표현하거나, 표본의 한계를 무시하거나, 과잉 일반화를 합니다. 규칙 파일이 빠르게 두꺼워집니다.### 학습 목표 (트랙 B 버전)

| 공통 목표 | 트랙 B에서의 구체적 표현 |
|----------|------------------------|
| 1. AI 생성물 한계 식별 | AI가 만든 분석 결과에서 논리적 비약, 과잉 일반화를 찾아낸다 |
| 2. 활용 방식별 품질 차이 | "분석해줘"와 구조화된 분석 요청의 결과 차이를 설명한다 |
| 3. AI 사용 여부 판단 | 데이터 해석은 직접, 코드 생성은 AI에게 — 근거 있는 역할 분배를 한다 |
| 4. 구조화된 명세 변환 | "급식 만족도가 낮다"를 측정 가능한 차원과 데이터 수집 계획으로 변환한다 |
| 5. 수용 기준 사전 정의 | "표본 20명 이상", "상관과 인과 구분" 같은 분석 품질 기준을 세운다 |
| 6. 실패 기반 규칙 구축 | "AI는 표본 크기 한계를 언급하지 않는다" 같은 규칙을 축적한다 |
| 7. 메타인지적 점검 | 분석 완료 후 "이 결론에 논리적 비약이 없는가?" 점검 루틴을 수행한다 |

### 8시간 차시별 구성

#### 차시 1-2: "직감 vs 데이터" (2시간)

| 시간 | 활동 | 핵심 |
|------|------|------|
| 00-15분 | 직감 설문: "우리 학교 급식 만족도는 몇 %?" | 학생별 추정치 수집 |
| 15-35분 | 실제 데이터와 비교 (교사 준비 또는 간이 조사) | 직감과 데이터의 괴리 체험 |
| 35-50분 | "같은 데이터, 다른 해석" 활동 | AI에게 같은 데이터를 다르게 분석하게 시키기 |
| 50-70분 | 문제 구조화 연습: "급식 만족도가 낮다" → 분해 | 맛/양/영양/대기시간/메뉴다양성 등 차원 분리 |
| 70-90분 | 개인 프로젝트 주제 선택 + 초기 데이터 수집 계획 | 학교생활 데이터 (아래 선택지 참고) |
| 90-100분 | 첫 규칙 추가 (도입부에서 가져온 규칙 파일에) | "데이터 분석에서 AI는..." |

**선생님 사전 준비 데이터셋 선택지** (학생이 직접 수집할 필요 없이 바로 분석에 들어갈 수 있도록):

| 데이터셋 | 내용 | 규모 | AI 실수가 잘 나오는 포인트 |
|----------|------|------|------------------------|
| A. 급식 만족도 | 메뉴별 만족도, 잔반량, 요일별 비교 | ~100행 | 상관→인과 혼동 (특정 메뉴일 때 잔반 적음 → "이 메뉴가 좋으니까"?) |
| B. 도서관 이용 | 월별 대출 수, 장르, 시간대, 학년별 | ~200행 | 시계열 과잉 해석 (시험 기간 대출↓ → "독서 흥미 하락"?) |
| C. 등교 교통 | 교통수단, 소요시간, 지각 여부, 날씨 | ~80행 | 표본 크기 무시 (자전거 5명 중 0명 지각 → "자전거가 최고"?) |

선생님이 세 데이터셋을 미리 준비하여 학생이 하나를 선택하게 합니다. 데이터 수집의 시간 부담을 줄이고, **분석과 검증**에 집중할 수 있게 합니다. 학생이 자기 데이터를 쓰고 싶다면 허용하되, 준비된 데이터셋 사용을 권장합니다.

#### 차시 3-4: "AI에게 분석 시키기" (2시간)

| 시간 | 활동 | 핵심 |
|------|------|------|
| 00-20분 | 분석 명세서 작성 (AI에게 시키기 전) | 데이터, 분석 방법, 기대 산출물 정의 |
| 20-25분 | 수용 기준 사전 정의 | "표본 크기 언급", "인과 주장 금지", "대안 해석 제시" |
| 25-50분 | AI에게 명세서 전달 → 분석 결과 생성 | |
| 50-70분 | 수용 기준 체크리스트 적용 | 합격/불합격 판정, 근거 기록 |
| 70-90분 | 불합격 항목 수정 요청 + 재검증 | "AI야 너 표본이 5명인데 일반화했어" |
| 90-100분 | 규칙 파일 업데이트 | |

수용 기준 예시:
```
[데이터 분석 수용 기준]
□ 필수: 표본 크기와 한계를 명시했는가
□ 필수: 상관관계를 인과관계로 표현하지 않았는가
□ 필수: 최소 하나의 대안 해석을 제시했는가
□ 선호: 시각화가 데이터를 왜곡하지 않는가
□ 선호: 결론이 데이터 범위를 넘지 않는가
```

#### 차시 5-6: "자유 프로젝트 구축" (2시간)

학생이 자기 주제로 전체 루프를 독립 수행합니다:
1. 문제 구조화 → 2. 데이터 수집 설계 → 3. 명세서 작성 → 4. 수용 기준 정의 → 5. AI에게 분석 시키기 → 6. 수용 기준으로 검증 → 7. 규칙 파일 업데이트

추천 주제: 급식 메뉴 만족도, 도서관 이용 패턴, 등교 교통수단과 지각률, 교실 온도와 집중도, 급식 잔반량 추이

#### 차시 7-8: "프로젝트 완성 + 교차 검증" (2시간)

| 시간 | 활동 | 핵심 |
|------|------|------|
| 00-30분 | 자유 프로젝트 마무리 | |
| 30-50분 | 교차 검증: 상대방 분석의 논리적 허점 찾기 | "네 분석에서 이 부분은 비약인 것 같아" |
| 50-70분 | 교차 검증 결과 반영 + 규칙 파일 업데이트 | |
| 70-90분 | 트랙 B 규칙 파일 최종 정리 | 범주별 정리 (명세 규칙, 검증 규칙, AI 실수 패턴) |
| 90-100분 | 트랙 B 성찰: "정답이 없는 영역에서 기준을 세운다는 것" | |

**트랙 B 종료 시 규칙 파일 규모:** 기존 10~20개 + 트랙 B 추가 5~10개 = 약 15~30개

---


---

## 7. 트랙 C: Pico 2W — 피지컬 컴퓨팅 (8시간)

### 왜 마지막 트랙인가

Pico가 마지막인 이유는 두 가지입니다:

**첫째, 기초 과제에서 AI가 너무 잘합니다.** LED 켜기, 버튼 제어 같은 과제는 AI가 거의 완벽하게 해결합니다. "AI 완벽하네, 끝" → 규칙 파일을 만들 동기가 없습니다. 하지만 이미 두 트랙을 경험한 학생은 "기초에서 잘한다고 방심하면 안 된다"는 감각이 있으므로, 스스로 더 복잡한 과제로 나아갑니다.

**둘째, 실패 원인이 세 곳에 분산됩니다.** 안 되면 하드웨어 문제인지, AI 코드 문제인지, 환경 문제인지 구분해야 합니다. 첫 교육에서 이 세 변수를 분리하는 능력은 없지만, 앞의 두 트랙에서 "AI 문제 vs 다른 문제를 분리하는" 경험을 쌓은 학생은 이 구분이 가능합니다.

### 트랙 C의 고유한 가치

물리 세계가 대신 검증합니다. LED가 켜지거나 안 켜집니다. 센서가 값을 보내거나 안 보냅니다. **해석의 여지가 없는 검증**입니다. 이것은 트랙 B(논리적 타당성)나 트랙 A(시각적/수치적 정확성)와 질적으로 다른 경험입니다.

### 학습 목표 (트랙 C 버전)

| 공통 목표 | 트랙 C에서의 구체적 표현 |
|----------|------------------------|
| 1. AI 생성물 한계 식별 | AI가 생성한 MicroPython 코드에서 핀 번호, 라이브러리, 문법 오류를 찾는다 |
| 2. 활용 방식별 품질 차이 | "LED 켜줘"와 구체적 핀·동작 명세의 결과 차이를 설명한다 |
| 3. AI 사용 여부 판단 | 회로 이해는 직접, 코드 생성은 AI에게 — 근거 있는 역할 분배를 한다 |
| 4. 구조화된 명세 변환 | "온도 측정기"를 센서 종류, 핀 연결, 데이터 형식, 표시 방법으로 구조화한다 |
| 5. 수용 기준 사전 정의 | "센서 값이 실제 온도와 ±2℃ 이내"같은 물리적 기준을 세운다 |
| 6. 실패 기반 규칙 구축 | "AI는 DHT11과 DHT22를 혼동한다"같은 규칙을 축적한다 |
| 7. 메타인지적 점검 | 코드 업로드 전 핀 번호 확인, 실행 후 물리적 동작 확인 루틴을 수행한다 |

### 8시간 차시별 구성

#### 차시 1-2: "물리 세계는 거짓말을 안 한다" (2시간)

| 시간 | 활동 | 핵심 |
|------|------|------|
| 00-15분 | 환경 세팅 (펌웨어, Thonny 연결) | 교사가 사전 설치 권장 |
| 15-30분 | LED 켜기 — 직접 코딩 | AI 없이 기본 체험 |
| 30-45분 | 외부 LED + 버튼 — 직접 코딩 | 회로+코드 통합 체험 |
| 45-60분 | 같은 과제를 AI에게 시키기 | (AI가 잘함 → "쉬운 건 잘하네") |
| 60-80분 | 온도 센서 읽기 — AI에게 시키기 | (AI가 실수하기 시작) |
| 80-95분 | 안 될 때 원인 분리: HW? SW? 환경? | **트랙 C의 핵심 기술** |
| 95-100분 | 규칙 파일에 Pico 규칙 추가 | "핀 번호를 반드시 확인" 등 |

#### 차시 3-4: "복잡해지면 AI도 틀린다" (2시간)

중간 복잡도 과제에서 AI 실수가 급증합니다: 센서 모델 혼동, 존재하지 않는 MicroPython 라이브러리, 잘못된 GPIO 핀.

**원인 분리 훈련:**
```
안 된다!
  ├─ 배선 확인: LED 직접 연결로 테스트
  ├─ 코드 확인: print()로 값 출력
  └─ 환경 확인: 펌웨어 버전, 라이브러리 존재 여부
```

#### 차시 5-6: "자유 프로젝트 구축" (2시간)

학생이 자기 주제로 전체 루프를 수행합니다.

추천 주제: 신호등 시뮬레이터, 디지털 온도계, 침입 감지 알람, 스마트 조명, WiFi 데이터 로거

#### 차시 7-8: "프로젝트 완성 + 트랙 C 규칙 정리" (2시간)

프로젝트 완성 + 교차 테스트 (상대방 프로젝트의 명세서만 보고 재현 시도) + 트랙 C 규칙 파일 정리

**트랙 C 종료 시 규칙 파일 규모:** 기존 15~30개 + 트랙 C 추가 5~10개 = 약 20~40개

---

## 8. 종합 성찰 (2시간)

### 목적

네 영역(챗봇, 데이터 분석, VPython, Pico)에서 축적한 규칙을 통합하고, **도메인을 넘어서는 원칙**을 추출합니다.

### 2시간 구성

#### 1시간 전반: 통합과 교환

**활동 1: 규칙 파일 통합 (20분)**

학생이 네 영역의 규칙을 하나로 합치고, 분류합니다:

```
[나의 AI 규칙 파일 — 최종]

## 어떤 AI 작업에든 적용되는 규칙 (공통)
- ...

## 데이터 분석에서의 규칙
- ...

## 3D 창작/시뮬레이션에서의 규칙
- ...

## 하드웨어/피지컬 컴퓨팅에서의 규칙
- ...

## 봇/대화형 AI에서의 규칙
- ...
```

**활동 2: 규칙 파일 교환 + 피드백 (20분)**

다른 학생의 규칙 파일을 읽고:
- "내가 발견 못한 규칙" 체크
- "이건 공통 규칙인데 특정 도메인에만 적었다" 지적
- 서로에게 규칙 추천

**활동 3: 학급 메타 규칙 TOP 5 추출 (20분)**

학급 전체에서 가장 많이 등장한 규칙을 모읍니다. 도입부 갤러리 워크의 빨간 포스트잇과 비교합니다.

> "4시간 전에 발견한 공통 규칙과, 지금 30시간 후에 발견한 공통 규칙이 어떻게 다른가요?"

도입부에서는 "모르면 모른다고 해", "범위 밖은 거절해" 같은 기본 규칙이었다면, 30시간 후에는 더 정교한 원칙이 추가됩니다:

```
[도입부에서 발견한 규칙]
- 모르면 모른다고 해
- 가정하지 말고 물어봐
- 범위 밖은 거절해

[30시간 후 추가된 규칙]
- AI에게 시키기 전에 내가 먼저 "정답"(또는 기대값)을 가지고 있어야 한다
- 명세가 정밀할수록 결과가 좋다 — 대충 시키면 대충 나온다
- AI의 실수 패턴은 도메인마다 다르지만, 규칙을 만드는 방법은 같다
- 수용 기준을 결과 보기 전에 세워야 한다 (사후에 세우면 기준이 결과에 끌려간다)
- 다른 사람이 테스트하면 내가 못 찾은 허점이 나온다
```

#### 1시간 후반: 성찰과 마무리

**활동 4: 성찰 에세이 (30분)**

"나의 AI 활용 규칙 파일" 1페이지 작성. 다음 질문에 답합니다:

```
① 30시간 동안 규칙이 몇 개에서 몇 개로 자랐는가?

② 가장 기억에 남는 AI의 실패와, 그 실패에서 태어난 규칙은?

③ 네 영역(챗봇, 데이터, VPython, Pico)에서 공통으로 적용된 규칙은?

④ Fowler의 질문에 답하기: "나의 엄격함은 어디로 이동했는가?"
   (코드를 타이핑하는 것에서 → _______________으로)

⑤ AI가 더 똑똑해지면, 이 규칙 파일은 필요 없어질까?
   아니면 더 중요해질까? 왜?
```

**활동 5: 최종 공유 (20분)**

자발적으로 성찰을 공유합니다.

**선생님 마무리 (10분):**

> "여러분이 30시간 동안 만든 것은 챗봇도 아니고, 시뮬레이션도 아니고, IoT 장치도 아닙니다.
> **규칙 파일**입니다.
>
> 챗봇은 사라질 수 있고, VPython은 안 쓸 수 있고, Pico는 서랍에 넣어둘 수 있습니다.
> 하지만 규칙 파일은 여러분이 AI와 일하는 한 계속 자랍니다.
>
> 오늘 이후로도, AI에게 뭔가를 시킬 때마다 한 줄씩 추가해보세요.
> 그 파일이 두꺼워질수록, 여러분은 AI의 사용자가 아니라 설계자가 됩니다."

---

## 9. 7개 교육 목표와 평가

### 목표 체계

| 층위 | 목표 | 핵심 질문 |
|------|------|----------|
| 인식·판단 | 1. AI 생성물 한계 식별 | AI가 준 결과에서 문제를 찾을 수 있는가? |
| 인식·판단 | 2. 활용 방식별 품질 차이 | 요청 방식에 따라 결과가 왜 다른지 설명할 수 있는가? |
| 인식·판단 | 3. AI 사용 여부 판단 | 어떤 일은 직접 하고 어떤 일은 AI에게 맡길지 결정할 수 있는가? |
| 구조화 | 4. 모호한 상황의 구조화 변환 | "대충"을 "정확히"로 바꿔서 AI에게 전달할 수 있는가? |
| 구조화 | 5. 수용 기준 사전 정의 | 결과를 보기 전에 "이러면 합격"을 정할 수 있는가? |
| 반복 개선 | 6. 실패 기반 규칙 구축 | AI가 실수할 때마다 "다시는 이러지 마"를 축적하는가? |
| 반복 개선 | 7. 메타인지적 점검 | 끝난 후 "내가 놓친 게 없나?" 스스로 돌아보는가? |

### 5단계 평가 척도

| 단계 | 이름 | 한 줄 기준 |
|:---:|------|----------|
| 1 | 미참여 | 아무것도 안 했다 |
| 2 | 시도 | 무엇이라도 했다 |
| 3 | 노력 | 열심히 했다 |
| 4 | 우수 | 잘했다 |
| 5 | 탁월 | 아주 잘했다 |

### 30시간 후 기대 분포 (25명 기준)

- **5단계 (탁월)**: ~3~5명 — 학급 최상위권. 규칙을 체계적으로 분류하고 도메인을 넘나드는 통찰을 보임
- **4단계 (우수)**: ~8~10명 — 명세와 기준을 꼼꼼히 세우고, 규칙 파일에 사건 기록이 충실
- **3단계 (노력)**: ~8~10명 — 활동에 적극 참여하고 규칙을 축적했으나 체계화가 부족
- **2단계 (시도)**: ~2~3명 — 참여는 했으나 형식적. 규칙이 피상적
- **1단계 (미참여)**: ~0~1명 — 산출물 미제출

### 평가 원칙

**"AI가 만든 것이 아니라, 학생이 설계한 구조를 평가한다."**

| AI가 만든 것 (평가 안 함) | 학생이 만든 것 (평가 대상) |
|--------------------------|--------------------------|
| 코드, 분석 결과, 시각화 | 명세서 (AI에게 시키기 전 작성) |
| 봇의 대화 내용 | 수용 기준 (결과 판정 기준) |
| 디버깅 제안 | 규칙 파일 (실패에서 축적한 규칙) |
| | 과정 기록 (AI 상호작용 로그) |
| | 성찰 에세이 |

### 평가 비중

| 도구 | 적용 시점 | 비중 |
|------|-----------|------|
| 명세서 + 수용 기준 | 각 트랙 | 30% |
| 규칙 파일 (누적 성장 포함) | 전 과정 | 25% |
| 과정 기록 | 전 과정 | 25% |
| 종합 성찰 에세이 | 마지막 차시 | 20% |

### 과정 기록 형식

```
[요청] 내가 AI에게 무엇을 요청했는가
[결과] AI가 무엇을 내놓았는가
[판정] 수용 / 기각 / 수정
[근거] 왜 그렇게 판정했는가
[교훈] 다음에는 어떻게 할 것인가 → (규칙 파일에 추가)
```

### 산출물별 5단계 채점 루브릭

> **사용법:** 학생 산출물을 받으면, 해당 산출물 유형의 루브릭에서 가장 잘 맞는 단계를 찾습니다.
> 두 단계 사이에 걸치면 낮은 단계를 줍니다 (학생이 다음 단계로 가려면 무엇이 필요한지 피드백하기 위해).

#### 루브릭 1: 명세서 (평가 비중 15%)

명세서는 AI에게 시키기 **전에** 학생이 작성하는 "무엇을 원하는지"의 구조화된 기술입니다.

| 단계 | 이름 | 기준 | 예시 (트랙 A 기준) |
|:---:|------|------|-----------------|
| 1 | 미참여 | 명세서를 작성하지 않음. AI에게 바로 시키고 끝냄 | (명세서 없음) |
| 2 | 시도 | "로봇 만들어줘" 수준의 한 줄, 또는 모호한 자연어 서술. AI가 알아서 해석해야 함 | "빨간 몸통에 파란 팔이 달린 로봇을 만들어줘" |
| 3 | 노력 | 핵심 요소(객체, 속성, 관계)를 나누어 기술. 구조화를 시도했으나 수치가 불완전하거나 일부 요소가 빠짐 | "몸통: 빨간 box, 팔: 파란 cylinder 2개(좌우 대칭), 다리: 초록 cylinder 2개" — 크기·위치 일부 누락 |
| 4 | 우수 | 모든 요소에 수치(좌표, 크기, 색상)를 포함. AI가 해석할 여지가 거의 없는 완전한 명세 | "몸통: 빨간 box(2×3×1) pos(0,0,0), 머리: 파란 sphere(r=0.8) pos(0,2.3,0), 왼팔: 초록 cylinder pos(-1.5,0.5,0) axis(0,-2,0) r=0.2" |
| 5 | 탁월 | 완전한 수치 명세 + 필수/선호 구분 + 제약 조건 또는 대안 시나리오를 포함 | 4단계 + "필수: 관통 금지, 좌우 대칭. 선호: 손가락 표현. 대안: 어려우면 구로 대체" |

**트랙별 변환 가이드:**
- **트랙 B**: "객체·좌표" → "분석 대상·변수·방법·기대 산출물" (예: "급식 만족도를 메뉴별·요일별로 교차 분석, 막대 차트로 시각화")
- **트랙 C**: "객체·좌표" → "센서·포트·동작·출력 형식" (예: "Grove 온도 센서를 A0 포트에 연결, 1초 간격으로 읽어서 LCD에 표시")

#### 루브릭 2: 수용 기준 (평가 비중 15%)

수용 기준은 AI 결과를 보기 **전에** 학생이 정하는 "합격/불합격 판정 기준"입니다.

| 단계 | 이름 | 기준 | 예시 |
|:---:|------|------|------|
| 1 | 미참여 | 수용 기준을 세우지 않음. AI 결과를 그냥 받아들임 | (기준 없음) |
| 2 | 시도 | 기준을 적었지만 주관적이거나 검증 불가능 | "로봇이 예뻐야 한다", "분석이 정확해야 한다" |
| 3 | 노력 | 검증 가능한 기준 2~3개를 사전에 정의. 체크리스트 형태로 작성 | "□ 팔이 몸통을 관통하지 않는다 □ 좌우 대칭이다 □ 색상이 내가 지정한 대로다" |
| 4 | 우수 | 검증 가능한 기준 4개 이상 + 필수/선호 구분 + 각 기준에 판정 방법 명시 | "필수: □ 관통 없음(눈으로 확인) □ 좌우 대칭(x좌표 비교). 선호: □ 비율 자연스러움" |
| 5 | 탁월 | 필수/선호 구분 + 기준 우선순위 + 미충족 시 대응 방안까지 설계 | 4단계 + "우선순위: 관통 > 대칭 > 색상. 관통 시 pos 재지정, 대칭 실패 시 좌표 부호 반전 요청" |

**트랙별 변환 가이드:**
- **트랙 B**: "□ 표본 크기와 한계를 명시했는가 □ 상관관계를 인과관계로 표현하지 않았는가 □ 대안 해석을 1개 이상 제시했는가"
- **트랙 C**: "□ 센서 값이 실제 측정값과 ±2℃ 이내 □ LED가 조건에 맞게 점등/소등 □ 시리얼 출력에 에러 없음"

#### 루브릭 3: 규칙 파일 (평가 비중 25%)

규칙 파일은 30시간 동안 실패 경험에서 축적한 "AI에게 다시는 이러지 마"의 누적 기록입니다.

| 단계 | 이름 | 기준 | 예시 |
|:---:|------|------|------|
| 1 | 미참여 | 규칙 파일이 없거나, 초기 상태 그대로 | (빈 파일 또는 템플릿 그대로) |
| 2 | 시도 | 규칙 몇 개를 적었지만 일반론 수준. 사건 기록 없음 | "AI를 잘 써야 한다", "AI는 가끔 틀린다" |
| 3 | 노력 | 규칙 5개 이상. 실패 사건에서 나온 구체적 규칙이 있음. 순서대로 나열 | "AI는 y축과 z축을 혼동한다 → 축 방향 명시 (사건: 팔이 앞으로 나옴)" — 나열식 |
| 4 | 우수 | 규칙 10개 이상 + 사건 기록 충실 + 범주별 분류(DON'T/DO/도메인별). 중복 제거됨 | 범주별 정리 + 사건 기록 + 도메인 섹션 구분 |
| 5 | 탁월 | 체계적 분류 + 도메인을 넘는 공통 규칙 식별 + 규칙의 수정·통합 흔적이 있음 | 4단계 + "공통: 시키기 전에 기대 결과를 먼저 적을 것" + 트랙 A 규칙을 트랙 C에 변형 적용한 기록 |

**채점 핵심:** 규칙 수보다 **"이 규칙이 생긴 사건"이 기록되어 있는지**가 중요합니다. 사건 없는 규칙은 "외워서 적은 것"이지 "경험에서 나온 것"이 아닙니다.

#### 루브릭 4: 과정 기록 (평가 비중 25%)

과정 기록은 AI와의 상호작용을 [요청→결과→판정→근거→교훈] 형식으로 남긴 로그입니다.

| 단계 | 이름 | 기준 | 예시 |
|:---:|------|------|------|
| 1 | 미참여 | 과정 기록이 없음 | (미제출) |
| 2 | 시도 | AI와의 대화를 복사하거나, "이상한 게 나왔다" 수준. [요청]과 [결과]만 간단히 기록 | "로봇 만들어달라고 했더니 이상한 게 나왔다" |
| 3 | 노력 | 5요소 중 3~4개를 기록. 판정은 있지만 근거가 약하거나, 교훈이 빠짐 | "[요청] 빨간 로봇 [결과] 파란 로봇 [판정] 기각 [근거] 색이 다름" — 교훈 누락 |
| 4 | 우수 | 5요소 모두 기록. 근거가 수용 기준과 연결됨. 교훈이 규칙 파일에 실제로 추가됨 | "[판정] 기각 — 수용 기준 '관통 금지' 위반 [근거] y좌표 < box 높이 [교훈] → 규칙 #7 추가" |
| 5 | 탁월 | 5요소 완비 + 여러 건의 기록에서 AI 실수의 공통 패턴을 발견하여 기술 | 4단계 + "기록 #3, #5, #8 공통: AI는 객체 간 거리에서 반지름을 빼먹는다 → '간격 계산 공식 제시' 규칙 필요" |

#### 루브릭 5: 종합 성찰 에세이 (평가 비중 20%)

종합 성찰 에세이는 30시간 마지막에 작성하는 1페이지 분량의 글입니다.

| 단계 | 이름 | 기준 | 핵심 판별 포인트 |
|:---:|------|------|----------------|
| 1 | 미참여 | 미제출이거나 "재미있었다" 한 줄 | 규칙·실패에 대한 언급 없음 |
| 2 | 시도 | 활동을 나열하고 감상을 적음. "왜"에 대한 성찰 없음 | "규칙이 20개가 되었다. VPython이 재미있었다" — 나열 |
| 3 | 노력 | 성찰 질문 3개 이상에 구체적으로 답함. 특정 실패 사건과 배운 것을 연결 | "AI가 팔을 머리에서 나오게 했을 때 좌표의 중요성을 알았다. 데이터 분석에서도 마찬가지였다" |
| 4 | 우수 | 도메인을 넘는 공통 원칙을 자기 언어로 설명. Fowler 질문에 구체적 답변 | "코드를 치는 엄격함이 '뭘 시킬지 정하는' 엄격함으로 바뀌었다. 네 영역 모두에서 이걸 느꼈다" |
| 5 | 탁월 | 공통 원칙 + "AI가 더 똑똑해져도 왜 필요한가"에 자기 답변이 있음 | "좌표 명시 같은 규칙은 사라질 수 있지만, '기준을 먼저 세우는 것'은 AI와 무관한 사고의 규칙이다" |

#### 채점 실전 가이드

**Step 1: 빠른 분류 (1인당 30초)**

먼저 "3단계(노력) 이상인가, 미만인가?"로 나눕니다.

```
3단계 이상의 신호:                    2단계 이하의 신호:
✓ 명세서에 구체적 요소가 나뉘어 있다   ✗ "로봇 만들어줘" 수준
✓ 수용 기준을 사전에 적었다            ✗ 기준 없이 "이상한데?" 수준
✓ 규칙에 실패 사건이 기록되어 있다     ✗ "AI를 잘 써야 한다" 수준
✓ 과정 기록에 판정과 근거가 있다       ✗ AI 대화 복사 수준
```

**Step 2: 4~5단계 판별 (1인당 1~2분)**

3단계 이상 학생만 4~5단계 여부를 봅니다.

```
4단계(우수)의 신호:
- 명세서에 수치가 빠짐없이 들어 있다
- 수용 기준에 필수/선호 구분이 있다
- 규칙이 범주별로 정리되어 있다
- 과정 기록의 교훈이 규칙 파일에 실제로 추가되어 있다

5단계(탁월)의 추가 신호:
- 도메인을 넘는 공통 규칙을 자기 언어로 식별했다
- 규칙을 수정·통합한 흔적이 있다 (처음 쓴 것과 최종이 다름)
- 성찰에서 "AI가 발전해도 변하지 않는 것"을 논했다
```

**Step 3: 피드백 (1인당 30초)**

"다음 단계로 가려면" 형식:

```
[2→3] "규칙에 '언제, AI가 뭘 틀렸는지' 사건을 적어보세요."
[3→4] "규칙을 DON'T/DO/검증으로 묶어보세요."
[4→5] "챗봇 규칙이 VPython에서도 통하는 것이 있나요? 그게 공통 규칙이에요."
```

---

## 10. 교사를 위한 안내

### 교사의 전제 조건

Hashimoto가 "스스로의 일을 재현하라"고 한 것은 교사에게도 적용됩니다.

1. **최소 조건**: AI에게 수업 자료 제작을 시켜본 경험이 있고, 그 결과에서 문제점을 발견한 적이 있어야 합니다.
2. **권장 조건**: 네 영역(챗봇, 데이터 분석, VPython, Pico) 중 최소 둘을 직접 수행해 본 경험
3. **이상적 조건**: 자기만의 규칙 파일을 만들어 본 경험

### 수업 운영 핵심 원칙

**1. 도입부를 절대 생략하지 마세요.**
챗봇 만들기에서 "실패 → 규칙 → 재시도" 루프를 체험하지 않으면, 이후 트랙에서 규칙 파일을 만드는 행위의 의미를 이해하지 못합니다.

**2. 실패를 환영하세요.**
규칙은 실패에서만 태어납니다. AI가 잘못된 결과를 내놓는 것은 학습 기회입니다. "AI가 이상한 결과를 줬다"는 보고를 들으면 "잘됐다, 규칙 하나 더 생겼네!"로 반응하세요.

**3. 종이를 쓰세요.**
실패 기록을 종이에 쓰는 것은 의도적 설계입니다. 화면에서 바로 수정하면 "생각 없이 복사"가 됩니다. 종이에 먼저 쓰고, 정리한 다음 코드/문서에 옮기는 과정이 규칙을 의식적으로 설계하는 훈련입니다.

**4. 학생 간 규칙 공유를 자주 하세요.**
다른 학생이 발견한 AI 실패 패턴은 교사의 강의보다 설득력 있습니다. 교차 테스트와 갤러리 워크를 충분히 활용하세요.

**5. 규칙 파일의 누적 성장을 추적하세요.**
도입부 5개 → 트랙 A 15개 → 트랙 B 25개 → 트랙 C 35개로 자라나는 것을 학생이 눈으로 볼 수 있게 하세요. 이 성장 자체가 학습의 증거입니다.

### "코딩을 가르치는 것"과 "규칙 설계를 가르치는 것"의 차이

이 교육과정은 기존 코딩 교육과 근본적으로 다릅니다. 이 차이를 이해하지 못하면 수업 중 무의식적으로 코딩 교육 방식으로 돌아가게 됩니다.

#### 무엇이 달라지는가

| | 기존 코딩 교육 | 이 교육과정 (규칙 설계) |
|--|---------------|----------------------|
| **학생이 만드는 것** | 작동하는 코드 | 규칙 파일 + 명세서 + 수용 기준 |
| **잘하는 학생의 모습** | 문법 오류 없이 코드를 짠다 | AI에게 시키기 전에 무엇을 원하는지 정확히 적는다 |
| **실패의 의미** | "틀렸다" → 고쳐야 한다 | "좋다, 규칙이 하나 생겼다" → 축적한다 |
| **교사의 역할** | 문법/로직 오류를 찾아 알려준다 | "지금 뭐가 이상한 것 같아요?"라고 묻는다 |
| **평가 기준** | 코드가 올바르게 동작하는가 | 명세가 정밀한가, 기준이 사전에 세워졌는가, 규칙이 실패에서 나왔는가 |
| **수업의 클라이맥스** | "드디어 돌아간다!" | "또 틀렸다! 근데 이번엔 왜 틀렸는지 안다" |

#### 교실에서 가장 흔한 실수 5가지

**실수 1: AI가 만든 코드를 디버깅해준다**

```
❌ "여기 3번째 줄에 인덴트 오류가 있어요, 고치세요."
✅ "AI가 준 코드가 안 돌아가네요. 어디가 문제인 것 같아요?
   그 문제가 생긴 이유가 뭘까요? 다음에 안 생기려면
   규칙을 어떻게 써야 할까요?"
```

코드 자체의 오류를 잡아주는 것은 코딩 교육입니다. 이 수업에서는 **"AI가 왜 이런 코드를 줬는지"**와 **"다음에 안 그러려면 규칙을 어떻게 쓸지"**를 묻는 것이 핵심입니다.

**실수 2: "좋은 규칙 목록"을 미리 알려준다**

```
❌ "AI에게 시킬 때는 이런 규칙들이 필요합니다: 1, 2, 3..."
✅ (학생이 AI 실패를 경험한 후) "방금 뭐가 이상했어요?
   그걸 규칙으로 쓰면 어떻게 될까요?"
```

규칙은 사전에 제공하는 것이 아니라 실패에서만 자라나야 합니다. 미리 알려주면 학생은 "외워서 적는" 것이지 "설계하는" 것이 아닙니다.

**실수 3: AI 결과물의 품질을 평가한다**

```
❌ "AI가 만든 3D 로봇이 꽤 잘 나왔네요. 90점!"
✅ "명세서에 '팔은 몸통 양옆에 대칭'이라고 썼는데,
   실제로 대칭인가요? 수용 기준에 합격인가요?"
```

AI가 만든 결과물의 멋짐/완성도는 평가 대상이 아닙니다. 학생이 사전에 세운 **수용 기준에 합격/불합격인지를 학생 스스로 판정했는가**가 중요합니다.

**실수 4: 코드를 이해시키려 한다**

```
❌ "sphere()가 뭔지 알아야 해요. 매개변수는 pos, radius, color..."
✅ "AI에게 '빨간 공을 (0,2,0)에 놓아줘'라고 시킬 건데,
   여기서 (0,2,0)이 뭔지만 알면 돼요."
```

이 수업에서 코드 문법은 학습 대상이 아닙니다. **학생이 알아야 하는 것은 "AI에게 뭘 시킬지"**이지, "AI가 만든 코드가 어떻게 동작하는지"가 아닙니다. 코드를 읽을 줄 아는 학생이 규칙 설계도 잘하는 경향은 있지만, 코드 이해가 전제 조건은 아닙니다.

**실수 5: "빨리 완성"을 독려한다**

```
❌ "시간 안에 로봇을 완성해야 해요, 서두르세요."
✅ "로봇이 아직 미완성이어도 괜찮아요. 지금까지
   규칙이 몇 개 생겼는지가 더 중요해요."
```

산출물(챗봇, 로봇, 분석 결과)의 완성도보다 **규칙 파일의 성장**이 중요합니다. 로봇이 절반만 됐어도 규칙이 10개 생겼으면 훌륭한 수업입니다.

#### 수업 중 자기 점검 질문

선생님이 수업 중에 스스로에게 물어볼 질문입니다:

```
┌─────────────────────────────────────────────────────────────┐
│ 수업 중 자기 점검                                             │
│                                                             │
│ □ 지금 내가 코드 문법을 설명하고 있지는 않은가?                  │
│ □ 학생이 AI 결과를 "맞다/틀리다"로 받아들이고 있지는 않은가?      │
│   → "네가 세운 기준으로 판단해봐"라고 돌려주고 있는가?           │
│ □ 학생이 규칙 없이 AI에게 여러 번 시키고 있지는 않은가?          │
│   → "먼저 종이에 규칙을 쓰고 나서 시켜봐"라고 안내했는가?        │
│ □ 오늘 수업에서 학생의 규칙 파일이 최소 1줄이라도 늘었는가?       │
│ □ AI가 실수한 순간을 "좋은 학습 기회"로 활용했는가?              │
│                                                             │
│ 하나라도 아니오가 있으면, 코딩 교육 모드로 돌아간 것입니다.       │
│ 괜찮습니다. 다시 "규칙 설계" 모드로 돌아오면 됩니다.             │
└─────────────────────────────────────────────────────────────┘
```

### 명세서와 규칙 파일의 구분 (교사 배경 지식)

학생에게 가르칠 필요는 없지만, 선생님이 이 구분을 알면 지도에 도움됩니다.

| | 명세서 (스캐폴딩) | 규칙 파일 (하네스) |
|--|-------------------|-------------------|
| **목적** | 인지적 깊이를 향해 탐색을 조직 | 실패의 재발을 차단 |
| **예시** | "빨간 box(2×3×1), 중심 (0,0,0)..." | "AI는 y축과 z축을 혼동한다 → 축 방향을 명시할 것" |
| **성격** | 이번 과제에 한정된 구조화 | 과제를 넘어 축적되는 원칙 |
| **교육에서** | 각 차시 활동에서 작성 (과제별 소멸) | 30시간 동안 계속 성장 (누적) |

학생이 "명세서"를 쓸 때는 **지금 이 과제를 정확히 전달하는 것**에 집중하고, "규칙 파일"에 추가할 때는 **다음에도 적용될 원칙**인지를 판단하게 합니다. 이 구분이 자연스럽게 되는 학생은 메타인지 수준이 높은 것입니다.

### API 비용 추정 가이드 (예산 승인용)

30시간 × 25명 기준 예상 비용입니다. Claude Sonnet 4 기준으로 산출했습니다.

| 트랙 | 1인당 예상 사용량 | 25명 기준 | 비고 |
|------|------------------|-----------|------|
| 도입부 (4시간) | ~$0.5 | ~$12.5 | 챗봇 대화 위주, 토큰 적음 |
| 트랙 A (8시간) | ~$1.5 | ~$37.5 | VPython 코드 생성, 중간 토큰 |
| 트랙 B (8시간) | ~$2.0 | ~$50.0 | 데이터 분석+시각화, 토큰 많음 |
| 트랙 C (8시간) | ~$1.5 | ~$37.5 | MicroPython 코드, 중간 토큰 |
| 종합 성찰 (2시간) | ~$0.5 | ~$12.5 | 정리 위주, 토큰 적음 |
| **합계** | **~$6.0** | **~$150** | |

**실전 팁:**
- Anthropic 콘솔에서 **spending limit을 $200~250**으로 설정하면 예상치의 1.5배 여유가 있습니다
- 학생이 "같은 질문 반복 전송"으로 토큰을 낭비하는 경우가 많습니다 → "보내기 전에 한 번 더 읽어보세요"를 습관화시키세요
- 학기 중 모델이 업데이트되면 가격이 변할 수 있으므로, 학기 초에 한 번 확인하세요

### AI 도구 변화에 대한 대응

학기 초에 가르친 AI의 한계가 학기 말에는 더 이상 한계가 아닐 수 있습니다.

도구가 변해도 변하지 않는 것에 무게를 두세요:
- 문제 구조화 능력
- 수용 기준 사전 정의 능력
- 결과 검증 능력
- 실패에서 규칙을 뽑아내는 능력

핵심 메시지: **"AI의 능력과 상관없이, 검증 기준을 세우는 것은 당신의 몫입니다."**

---

## 11. 문서 구조

```
[새 버전]
05_총론_AI시대의규칙설계자_30시간.md       ← 이 문서 (전체 구조와 철학)
01_트랙A_VPython_3D창작과시뮬레이션.md     ← VPython 8시간 상세 (작성 완료)
02_트랙B_일상문제해결_프로젝트.md          ← 데이터 분석 8시간 상세
03_트랙C_Pico2W_피지컬컴퓨팅.md           ← Pico 2W 8시간 상세
04_워크숍_나의첫_규칙파일.md              ← 도입부 3시간 워크숍 (작성 완료)
```

각 트랙 문서에는 다음이 포함됩니다:
- 차시별 상세 계획 (타임라인, 활동, 자료)
- 예시 프로젝트와 구체적 과제
- 수용 기준 예시
- 규칙 파일 성장 예시
- 예상 Q&A와 트러블슈팅
- 평가 루브릭 (7개 목표별)
- 교사 준비 사항

---

## 부록: 검증 스펙트럼

30시간에 걸쳐 학생이 경험하는 검증 방법의 진행:

```
도입 (챗봇)    트랙 A 전반      트랙 A 후반     트랙 B (데이터)   트랙 C (Pico)
대화 상식  →   시각적 검증   →  수치적 검증   →  논리적 판단  →  물리적 동작
                                                              
"이 봇        "이 모양이       "이 값이        "이 분석에     "LED가
 이상해"       내가 원한 건     이론값과        비약이 있어"   켜지냐
                              아닌데"          ±5%냐"                       안 켜지냐"

검증 부담:     검증 부담:       검증 부담:      검증 부담:     검증 부담:
최저           낮음             중간            낮음~중간       중간+변수분리

사전 지식:     사전 지식:       사전 지식:      사전 지식:     사전 지식:
없음           눈               물리 공식       일상 상식+논리  HW+SW+환경
```

이 스펙트럼을 따라가면서, 학생은 **"규칙을 만드는 방법은 같지만, 검증 방법은 도메인마다 다르다"**는 메타 인사이트에 도달합니다.
